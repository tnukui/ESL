\documentclass{jsarticle}


\usepackage{ascmac}
\usepackage[top=30truemm,bottom=30truemm,left=25truemm,right=25truemm]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath,amssymb}
\usepackage[dvipdfmx]{graphicx}
\usepackage{cases}

\begin{document}

\title{The Elements of Statistical Learning\\統計的学習の基礎}
\author{nukui}
\date{\today}
\maketitle



\section{序章}
\section{教師あり学習の概要}
%%% Ex.2.1
\subsection{}
%\begin{shadebox}
%$K$クラス分類問題の目標変数$t_k$が、第$k$要素のみが$1$で他の要素は$0$である$K$次元ベクトルによって表されているとする。予測結果$\hat{y}$を全ての要素の和が$1$となるように正規化したとき、$\hat{y}$の最大要素を持つクラスへの分類と$||t_k-\hat{y}||$を最小化するクラスへの分類が等価であることを示せ。
%\end{shadebox}
%$\hat{y}$の最大要素が第$j$要素$\hat{y_j}$であるとする。
%\begin{align*}
%&\ \ \ ||t_k-\hat{y}||^2-||t_j-\hat{y}||^2\\
%&=\{\sum_{i\neq k}\hat{y_i}^2+(1-\hat{y_k})^2\}-\{\sum_{i\neq j}\hat{y_i}^2+(1-\hat{y_j})^2\}\\
%&=\hat{y_j}^2+(1-\hat{y_k})^2-\hat{y_k}^2-(1-\hat{y_j})^2\\
%&=2(\hat{y_j}-\hat{y_k})\geq0
%\end{align*}
%よって、$k=j$のとき、$||t_k-\hat{y}||$が最小になることがわかる。以上より、$\hat{y}$の最大要素を持つクラスへの分類$j$と、$||t_k-\hat{y}||$を最小化するクラスへの分類が等価であることがわかる。


%%% 2.2
\subsection{}
\begin{shadebox}
図2.5の試行の例においてベイズ決定境界を求めよ。
\end{shadebox}
（青色クラス）2次元ガウス分布$N((1,0)^T,\bf{I})$から生成された10個の平均ベクトル（青色クラス）を$\{m_1,m_2,...,m_{10}\}$とし、2次元ガウス分布$N((0,1)^T,\bf{I})$から生成された10個の平均ベクトル（オレンジ色クラス）を$\{n_1,n_2,...,n_{10}\}$とする。
\subsubsection{}
$\{m_1,m_2,...,m_{10}\}$と$\{n_1,n_2,...,n_{10}\}$の値がすでにわかっていると仮定する。
このとき、ベイズ決定境界上の点$x$の条件は
\[\Pr(青色|x)=\Pr(オレンジ色|x)\]
となる。
\[\frac{\Pr(青色|x)}{\Pr(オレンジ色|x)}=\frac{\Pr(青色|x)\Pr(x)}{\Pr(オレンジ色|x)\Pr(x)}=\frac{\Pr(x|青色)\Pr(青色)}{\Pr(x|オレンジ色)\Pr(オレンジ色)}\]
$\Pr(青色)=\Pr(オレンジ色)$なので、結局、ベイズ決定境界の式は
\[\Pr(x|青色)=\Pr(x|オレンジ色)\]
と表せる。
10個の平均ベクトルのどれが選ばれるかは等確率であり、$i$番目のベクトルが選ばれた時には、平均$m_i$(または$n_i$)で、分散$\bf{I}/5$の２変数ガウス分布に従うので、ベイズ決定境界上の点$x$の条件式は
\[\sum_{i=1}^{10}\frac{1}{10}\frac{\sqrt{5}}{2\pi}\exp\{-\frac{5(x-m_i)^T(x-m_i)}{2}\}=\sum_{i=1}^{10}\frac{1}{10}\frac{\sqrt{5}}{2\pi}\exp\{-\frac{5(x-n_i)^T(x-n_i)}{2}\}\]
と表される。
\subsubsection{}
$\{m_1,m_2,...,m_{10}\}$と$\{n_1,n_2,...,n_{10}\}$の値が未知だと仮定した場合、観測された値を頼りに確率を計算することができる。
$N$個の$p$次元ベクトル$x_i\ (i=1,...,N)$が青色の点として観測されていて、$y_i\ (i=1,...,N)$がオレンジ色の点として観測されているとする。平均$\mu$で、分散$\sigma$の２変数ガウス分布の$x$における確率密度関数を$f(x;\mu,\sigma)$と表すとすると、
\begin{align*}
&\ \ \Pr(x\ |\ \{x_1,x_2,...,x_N\},青色)\\
&=\sum_{m_1,...,m_{10}}\Pr(\{m_1,...,m_{10}\}\ | \{x_1,x_2,...,x_N\})\Pr(x|\{m_1,...,m_{10}\})\\
&=\sum_{m_1,...,m_{10}}\frac{\Pr(\{m_1,...,m_{10}\})\Pr(\{x_1,...,x_N\}|\{m_1,...,m_{10}\})}{\Pr(\{x_1,x_2,...,x_N\})}\Pr(x|\{m_1,...,m_{10}\})\\
&=\sum_{m_1,...,m_{10}}\frac{\{\prod_{i=1}^{10}f({m_i};(1,0)^T,{\bf I})\}\{\prod_{k=1}^N\sum_{j=1}^{10}\frac{1}{10}f(x_k;{m_j},{\bf I}/5)\}}
{\int\{\prod_{i=1}^{10}f({m_i}';(1,0)^T,{\bf I})\}\{\prod_{k=1}^N\sum_{j=1}^{10}\frac{1}{10}f(x_k;{m_j}',{\bf I}/5)\}d{m_1}'...d{m_{10}}'}
\Pr(x|\{m_1,...,m_{10}\})\\
&=\int\{\frac{\{\prod_{i=1}^{10}f({m_i};(1,0)^T,{\bf I})\}\{\prod_{k=1}^N\sum_{j=1}^{10}\frac{1}{10}f(x_k;{m_j},{\bf I}/5)\}\{\sum_{j=1}^{10}\frac{1}{10}f(x;m_j,{\bf I}/5)\}}
{\int\{\prod_{i=1}^{10}f({m_i}';(1,0)^T,{\bf I})\}\{\prod_{k=1}^N\sum_{j=1}^{10}\frac{1}{10}f(x_k;{m_j}',{\bf I}/5)\}d{m_1}'...d{m_{10}}'}\}dm_1...dm_{10}
\end{align*}
となる。また、ベイズ決定境界上の点$x$の条件式は
\[\Pr(x | \{x_1,x_2,...,x_N\},青色)＝\Pr(x |\{y_1,y_2,...,y_N\},オレンジ色)\]
と表される。これを計算機でいい感じに求めることができるのかどうか、知りませんが。。。


%Ex.2.3
\subsection{}

\begin{shadebox}
式(2.24)を導け。
\[d(p,N)=(1-(\frac{1}{2})^{\frac{1}{N}})^{\frac{1}{p}}\]
\end{shadebox}
p次元空間の半径$r$の球の体積を$Cr^p$とおく($C$は定数)。半径1の球に$N$個の点が均一に散らばっているとすると、原点に最も近い点$x$が半径$r$の中に入っている確率は
\begin{align*}
\Pr(X<r)&=1-\Pr(X\geq r)\\
&=1-(\frac{C-Cr^p}{C})^N\\
&=1-(1-r^p)^N
\end{align*}
よって、$X$の中央値$d$は$\Pr(X<d)=\frac{1}{2}$となる境界なので、
\begin{align*}
1-(1-d^p)^N&=\frac{1}{2}\\
\frac{1}{2}&=(1-d^p)^N\\
1-d^p&=(\frac{1}{2})^\frac{1}{N}\\
d&=(1-(\frac{1}{2})^\frac{1}{N})^\frac{1}{p}
\end{align*}
以上より、$d(p,N)=(1-(\frac{1}{2})^\frac{1}{N})^\frac{1}{p}$と求められた。


%Ex.2.4
\subsection{}

%Ex.2.5
\subsection{}
\begin{shadebox}
\begin{enumerate}
\item[(a)]
式(2.27)を導出せよ。
\item[(b)]
式(2.28)を導出せよ。
\end{enumerate}
\end{shadebox}

\begin{enumerate}
\item[(a)]
\begin{align*}
{\rm EPE}(x_0)&=\mathbb{E}_{y_0|x_0}[\mathbb{E}_{\cal T}[(y_0-\hat{y}_0)^2]]\\
&=\mathbb{E}_{y_0|x_0}[\mathbb{E}_{\cal T}[y_0^2-2y_0 \hat{y}_0+\hat{y}_0^2]]
\end{align*}
ここで、$y_0$はトレーニングデータ${\cal T}$には依存せず、予測値$\hat{y}_0$は真の値$y_0$には依存しないので、
\begin{align*}
{\rm EPE}(x_0)&=\mathbb{E}_{y_0|x_0}[y_0^2]-2\mathbb{E}_{y_0|x_0}[y_0] \mathbb{E}_{\cal T}[\hat{y}_0]+\mathbb{E}_{\cal T}[\hat{y}_0^2]
\end{align*}
分散と平均の関係$\mathbb{E}(x^2)={\rm Var(x)}+{\mathbb{E}(x)}^2$を用いることで、
\begin{align*}
{\rm EPE}(x_0)&={\rm Var}(y_0|x_0)+{\mathbb{E}_{y_0|x_0}[y_0]}^2+{\rm Var}_{\cal T}(\hat{y}_0)+{\mathbb{E}_{\cal T}[\hat{y}_0]}^2-2\mathbb{E}_{y_0|x_0}[y_0] \mathbb{E}_{\cal T}[\hat{y}_0]\\
&={\rm Var}(y_0|x_0)+{\rm Var}_{\cal T}(\hat{y}_0)+({\mathbb{E}_{y_0|x_0}[y_0]}-{\mathbb{E}_{\cal T}[\hat{y}_0]})^2
\end{align*}
ここで${\rm Var}(y_0|x_0)$は真の値の分散$\sigma^2$である。\\
$|\mathbb{E}_{y_0|x_0}[y_0]-\mathbb{E}_{\cal T}[\hat{y}_0]|$はBiasと呼ばれる値で、真の値の期待値$\mathbb{E}_{y_0|x_0}[y_0]$とモデルの予測した値の期待値$\mathbb{E}_{\cal T}[\hat{y}_0]$の間の乖離を示す。最小二乗推定は不偏であることが知られており、Biasは0になる。\\
\\
以下では、${\rm Var}_{\cal T}(\hat{y}_0)$を計算する。\\
トレーニングデータ${\cal T}$は入力${\bf X}$と出力$y$の組み合わせとして表現できるので、${\cal T}=({\bf X},y)$と書ける。
\begin{align*}
{\rm Var}_{\cal T}(\hat{y}_0)&=\mathbb{E}_{({\bf X},y)}[\hat{y}_0^2]-{\mathbb{E}_{({\bf X},y)}[\hat{y}_0]}^2\\
&=\int \{\Pr({\bf X},y)\hat{y}_0^2 \}d{\bf X}dy-(\int\{ \Pr({\bf X},y)\hat{y}_0 \}d{\bf X}dy)^2\\
&=\int \{\Pr({\bf X})(\int \{\Pr(y|{\bf X})\hat{y}_0^2 \}dy\})\}d{\bf X}-(\int \{\Pr({\bf X})(\int \{\Pr(y|{\bf X})\hat{y}_0 \}dy\})\}d{\bf X})^2\\
&=\int \{\Pr({\bf X})\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0^2]\}d{\bf X}-(\int \{\Pr({\bf X})\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0]\}d{\bf X})^2\\
&=\int \{ \Pr({\bf X})({\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0) +{\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0]}^2)\}d{\bf X}
-(\int \{\Pr({\bf X})\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0]\}d{\bf X})^2\\
&=\int \{\Pr({\bf X}) {\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)\}d{\bf X} +
 \int\{\Pr({\bf X}){\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0]}^2\}d{\bf X}
-(\int \{\Pr({\bf X})\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0]\}d{\bf X})^2\\
&=\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)]
+\mathbb{E}_{{\bf X}}[(\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0])^2]
-(\mathbb{E}_{{\bf X}}[\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0]])^2\\
&=\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)]
+{\rm Var}_{{\bf X}}(\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0])
\end{align*}
以下では、$\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)]$と${\rm Var}_{{\bf X}}(\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0])$を計算していく。まず、$\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)]$を展開すると
\begin{align*}
\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)]&=\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(x_0^T\hat{\beta})]\\
&=\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)]\\
&=\mathbb{E}_{{\bf X}}[\mathbb{E}_{\Pr(y|{\bf X})}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)^2]-(\mathbb{E}_{\Pr(y|{\bf X})}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)])^2]\\
&=\mathbb{E}_{{\bf X}}[\mathbb{E}_{\Pr(y|{\bf X})}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)^T]\\
&\ \ \ \ \ \ \ -(\mathbb{E}_{\Pr(y|{\bf X})}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)])(\mathbb{E}_{\Pr(y|{\bf X})}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)^T])]\\
&=\mathbb{E}_{{\bf X}}[\mathbb{E}_{\Pr(y|{\bf X})}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)(y^T{\bf X}({\bf X}^T{\bf X})^{-1}x_0)]\\
&\ \ \ \ \ \ \ -(\mathbb{E}_{\Pr(y|{\bf X})}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^Ty)])(\mathbb{E}_{\Pr(y|{\bf X})}[y^T{\bf X}({\bf X}^T{\bf X})^{-1}x_0])]\\
&=\mathbb{E}_{{\bf X}}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^T\mathbb{E}_{\Pr(y|{\bf X})}[yy^T]{\bf X}({\bf X}^T{\bf X})^{-1}x_0)\\
&\ \ \ \ \ \ \ -(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^T\mathbb{E}_{\Pr(y|{\bf X})}[y]\mathbb{E}_{\Pr(y|{\bf X})}[y^T]{\bf X}({\bf X}^T{\bf X})^{-1}x_0)]\\
&=\mathbb{E}_{{\bf X}}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^T(\mathbb{E}_{\Pr(y|{\bf X})}[yy^T]-\mathbb{E}_{\Pr(y|{\bf X})}[y]\mathbb{E}_{\Pr(y|{\bf X})}[y^T]){\bf X}({\bf X}^T{\bf X})^{-1}x_0)]
\end{align*}
観測値$y_i$が互いに独立で、分散$\sigma^2$を持つとすると$\mathbb{E}_{\Pr(y|{\bf X})}[yy^T]-\mathbb{E}_{\Pr(y|{\bf X})}[y]\mathbb{E}_{\Pr(y|{\bf X})}[y^T]=\sigma^2 I$なので、
\begin{align*}
\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)]&=\mathbb{E}_{{\bf X}}[(x_0^T ({\bf X}^T{\bf X})^{-1}{\bf X}^T(\sigma^2 I){\bf X}({\bf X}^T{\bf X})^{-1}x_0)]\\
&=\mathbb{E}_{{\bf X}}[x_0^T ({\bf X}^T{\bf X})^{-1}x_0\sigma^2]
\end{align*}
ここで、${\cal T}=({\bf X},y)$であり、またカッコの中に$y$に依存する項目が出てこないので、
\[\mathbb{E}_{{\bf X}}[{\rm Var}_{\Pr(y|{\bf X})}(\hat{y}_0)]=\mathbb{E}_{{\bf X}}[x_0^T ({\bf X}^T{\bf X})^{-1}x_0\sigma^2]=\mathbb{E}_{\cal T}[x_0^T ({\bf X}^T{\bf X})^{-1}x_0\sigma^2]\]
次に、${\rm Var}_{{\bf X}}(\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0])$を計算していく。
\begin{align*}
{\rm Var}_{{\bf X}}(\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0])&={\rm Var}_{{\bf X}}(\mathbb{E}_{\Pr(y|{\bf X})}[x_0^T\hat{\beta}])\\
&={\rm Var}_{{\bf X}}(\mathbb{E}_{\Pr(y|{\bf X})}[x_0^T({\bf X}^T{\bf X})^{-1}{\bf X}^Ty])\\
&={\rm Var}_{{\bf X}}(x_0^T({\bf X}^T{\bf X})^{-1}{\bf X}^T{\bf X}\beta)\\
&={\rm Var}_{{\bf X}}(x_0^T\beta)
\end{align*}
これは${\bf X}$に依存しない定数なので、${\rm Var}_{{\bf X}}(\mathbb{E}_{\Pr(y|{\bf X})}[\hat{y}_0])=0$
\\
結局、${\rm EPE}(x_0)=\sigma^2+\mathbb{E}_{\cal T}[x_0^T ({\bf X}^T{\bf X})^{-1}x_0\sigma^2]$という(2.27)式が導かれた。




\item[(b)]
$N$が大きく${\cal T}$がランダムに選択されており、$\mathbb{E}_{\cal T}(X)=0$であるとする\footnote{${\bf X}$は観測された値を行とする行列を表しており、$X$は一つの観測値の確率変数を表す。$X$は$p$次元のベクトル。}。${\bf X}^T{\bf X}\to N{\rm Cov}(X)$となるので、
\begin{align*}
\mathbb{E}_{x_0}[{\rm EPE}(x_0)]&=\mathbb{E}_{x_0}[x_0^T \mathbb{E}_{\cal T}[({\bf X}^T{\bf X})^{-1}]x_0]\sigma^2+\sigma^2\\
&\sim\mathbb{E}_{x_0}[x_0^T (N{\rm Cov}(X))^{-1}x_0]\sigma^2+\sigma^2\\
&=\mathbb{E}_{x_0}[x_0^T ({\rm Cov}(X))^{-1}x_0]\sigma^2/N+\sigma^2
\end{align*}
トレース演算の巡回性$[{\rm trace}(AB)={\rm trace}(BA)]$および線形性により
\begin{align*}
\mathbb{E}_{x_0}[{\rm EPE}(x_0)]&\sim{\rm trace}[\mathbb{E}_{x_0}[x_0^T ({\rm Cov}(X))^{-1}x_0]]\sigma^2/N+\sigma^2\\
&=\mathbb{E}_{x_0}[{\rm trace}[x_0^T ({\rm Cov}(X))^{-1}x_0]]\sigma^2/N+\sigma^2\\
&=\mathbb{E}_{x_0}[{\rm trace}[ ({\rm Cov}(X))^{-1}x_0x_0^T]]\sigma^2/N+\sigma^2\\
&={\rm trace}[ ({\rm Cov}(X))^{-1}\mathbb{E}_{x_0}[x_0x_0^T]]\sigma^2/N+\sigma^2
\end{align*}
$\mathbb{E}_{\cal T}[X]=0$より$\mathbb{E}_{x_0}[x_0]=0$である。$\mathbb{E}_{x_0}[x_0x_0^T]=\mathbb{E}_{x_0}[(x_0-\mathbb{E}_{x_0}[x_0])(x_0-\mathbb{E}_{x_0}[x_0])^T]={\rm Cov}(x_0)$となるので、
\begin{align*}
\mathbb{E}_{x_0}[{\rm EPE}(x_0)]&\sim{\rm trace}[ ({\rm Cov}(X))^{-1}{\rm Cov}(x_0)]\sigma^2/N+\sigma^2\\
&={\rm trace}[ ({\rm Cov}(X))^{-1}{\rm Cov}(x_0)]\sigma^2/N+\sigma^2\\
&={\rm trace}[{\rm I}]\sigma^2/N+\sigma^2\\
&=p\sigma^2/N+\sigma^2
\end{align*}
以上により、式(2.28)が導かれた。
\end{enumerate}







%2.6
\subsection{}

%2.7
\subsection{}

%2.8
\subsection{}

%2.9
\subsection{}
\begin{align*}
\mathbb{E}_{\{X,y\}}[R_{\rm tr}(\hat{\beta})]&=\mathbb{E}_{\{X,y\}}[\min_{\beta}R_{\rm tr}(\beta)]\\
&\leq\min_{\beta}\mathbb{E}_{\{X,y\}}[R_{\rm tr}(\beta)]\\
&=\min_{\beta}\mathbb{E}_{\{\tilde{X},\tilde{y}\}}[R_{\rm te}(\beta)]\\
&\leq \mathbb{E}_{\{\tilde{X},\tilde{y}\}}[R_{\rm te}(\hat{\beta})]
\end{align*}
上記のそれぞれの等号や不等号の変形を証明すれば良い。\\
\begin{enumerate}
\item
$\mathbb{E}_{\{X,y\}}[R_{\rm tr}(\hat{\beta})]=\mathbb{E}_{\{X,y\}}[\min_{\beta}R_{\rm tr}(\beta)]$は定義より成り立つ。
\item
それぞれの値を$\beta$で最小化してから期待値（平均値）を取った方が、平均値を取った後に$\beta$で最小化するよりも小さいか等しくなるので、$\mathbb{E}_{\{X,y\}}[\min_{\beta}R_{\rm tr}(\beta)]\leq\min_{\beta}\mathbb{E}_{\{X,y\}}[R_{\rm tr}(\beta)]$が成り立つ。
\item
\begin{align*}
\min_{\beta}\mathbb{E}_{\{X,y\}}[R_{\rm tr}(\beta)]&=
\min_{\beta}\mathbb{E}_{\{X,y\}}[\frac{1}{N}\sum_{i=1}^{N}(y_i-\beta^{\mathrm{T}}x_i)^2]\\
&=\min_{\beta}\mathbb{E}_{\{x,y\}}[(y-\beta^{\mathrm{T}}x)^2]\\
&=\min_{\beta}\mathbb{E}_{\{\tilde{x},\tilde{y}\}}[(\tilde{y}-\beta^{\mathrm{T}}\tilde{x})^2]\\
&=\min_{\beta}\mathbb{E}_{\{\tilde{X},\tilde{y}\}}[\frac{1}{M}\sum_{i=1}^{M}(\tilde{y_i}-\beta^{\mathrm{T}}\tilde{x_i})^2]\\
&=\min_{\beta}\mathbb{E}_{\{\tilde{X},\tilde{y}\}}[R_{\rm te}(\beta)]
\end{align*}
より$\min_{\beta}\mathbb{E}_{\{X,y\}}[R_{\rm tr}(\beta)]=\min_{\beta}\mathbb{E}_{\{\tilde{X},\tilde{y}\}}[R_{\rm te}(\beta)]$が成り立つ。
\item
$\min_{\beta}$という関数の定義から$\min_{\beta}\mathbb{E}_{\{\tilde{X},\tilde{y}\}}[R_{\rm te}(\beta)]\leq \mathbb{E}_{\{\tilde{X},\tilde{y}\}}[R_{\rm te}(\hat{\beta})]$が成り立つ。
\end{enumerate}






















\end{document}